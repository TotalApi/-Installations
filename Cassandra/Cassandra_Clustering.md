Организация кластера Cassandra
==============================

Создание кластера Cassandra
---------------------------
Существует два режима организации кластера.

1. `SimpleSnitch` - если все сервера кластера находятся в одной локальной сети или кластера вообще нет.
    
	Этот режим устанавливается по умолчанию при установке Cassandra. 
	Требует таких настроек в файле `cassandra.yaml`:
	
		listen_address: 192.168.0.1        # Внутренний адрес сервера (адрес в локальной сети)

		endpoint_snitch: SimpleSnitch

		seed_provider:
			- class_name: org.apache.cassandra.locator.SimpleSeedProvider
			  parameters:
			  - seeds: "192.168.0.1"

	Сразу после создания системной БД в таком режиме текущий узел будет иметь значение датацентра равное `datacenter1`.
	

2. `GossipingPropertyFileSnitch` - при организации распределённого кластера если сервера кластера находятся в разных сетях.

	Для установки этого режима требуется выполнить несколько дополнительных действий: 
	
	Изменить настройки в файле `cassandra.yaml`:
	
		listen_address: 192.168.0.1          # Внутренний адрес сервера (за NAT - адрес в локальной сети)
		broadcast_address: xxx.xxx.xxx.1     # Внешний адрес сервера (перед NAT - адрес маршрутизатора)
		broadcast_rpc_address: xxx.xxx.xxx.1 # Внешний IP-адрес для взаимодействия с клиентами и другими узлами через RPC (Remote Procedure Call). Установите его в публичный IP-адрес, если Cassandra должна быть доступна снаружи.
		rpc_address: 0.0.0.0                 # Адрес, по которому будет доступна ваша Cassandra для клиентских подключений. Используйте 0.0.0.0 для того, чтобы Cassandra слушала все IP-адреса, или настройте на внутренний IP, если нужно.

		endpoint_snitch: GossipingPropertyFileSnitch

		seed_provider:
			- class_name: org.apache.cassandra.locator.SimpleSeedProvider
			  parameters:
			  - seeds: "xxx.xxx.xxx.1,xxx.xxx.xxx.2,...,xxx.xxx.xxx.10"

	Пробросить необходимые порты `7000/7001`, `9042` за NAT. Если NAT отсутствует, то локальные и внешние адреса могут совпадать.
	
	Каждый узел кластера должен иметь возможность подключится ко всем серверам кластера по этим портам.
	Проверить доступность сервера можно при помощи команды:

		nc -zv <внешний_IP_Cassandra> 7000
		nc -zv <внешний_IP_Cassandra> 9042

		
Добавление узла в распределённый кластер
----------------------------------------
Для добавление нового узла в кластер необходимо выполнить следующие действия:

1. Сервер нового узла кластера должен быть доступным через внешний белый IP-адрес.
2. Изменить файл `cassandra.yaml` для работы с `GossipingPropertyFileSnitch` (см. выше).
3. На всех узлах кластера в файле `cassandra.yaml` перечислить все внешние адреса узлов в разделе `seed_provider.parameters.seeds`
4. Если новый узел был до этого обычным узлом (получил при создании значение датацентра равное `datacenter1`) 
   изменить в файле `cassandra-rackdc.properties` значение `dc=dc1` на `dc=datacenter1`

Инициирование репликации для нового узла
----------------------------------------
После подключения нового узла к кластеру Cassandra, репликация данных не начинается автоматически. Нужно инициировать процесс вручную.
Для этого нужно:

1. Проверить статус узла в кластере
   С любого узла выполните:
		
		nodetool status

   Если новый узел успешно добавлен, он должен отображаться со статусом UN (Up, Normal).

Если его нет в списке, убедитесь, что:

- В cassandra.yaml указаны правильные `seed nodes`.
- `listen_address` и `broadcast_address` настроены корректно.
- `auto_bootstrap: false` НЕ указан (он нужен только для пустых узлов).

2. Перераспределить токены (если вручную настраивалась `num_tokens`)

Если у вас `num_tokens` > 1, запустите балансировку:
	
	nodetool cleanup

Это освободит ненужные данные на старых узлах.

3. Форсировать синхронизацию данных
   Выполните:

		nodetool repair

   Это заставит узел синхронизировать недостающие данные с другими узлами.

   Если много данных, можно ограничить диапазон репликации:

		nodetool repair -pr

   (`-pr` = только primary range для этого узла)

4. Проверить репликацию
   Запустите на любом узле:

		SELECT * FROM system.peers;

   Если новый узел появился в списке, значит он корректно присоединился.
   Также проверьте репликацию для конкретной таблицы:

		SELECT * FROM system.size_estimates WHERE keyspace_name = 'your_keyspace';

5. Настроить балансировку нагрузки (если кластер в эксплуатации)
   После репликации можно перераспределить нагрузку:

		nodetool move <new_token>

   Примечание: Это нужно, если токены вручную настраиваются.

Итого:
- Проверить статус узла (nodetool status).
- Запустить очистку (nodetool cleanup).
- Инициировать репликацию (nodetool repair).
- Проверить данные (system.peers, system.size_estimates).

После этих шагов новый узел начнёт полноценную репликацию в кластере.


Удаление узла из кластера
-------------------------
Удаление узла из кластера Cassandra требует нескольких шагов, чтобы избежать потери данных и нарушения работы кластера.

1. Проверить статус узла
   На любом узле выполните:
		
		nodetool status

   Запомните IP-адрес или имя узла, который нужно удалить. Он должен быть в состоянии "UN" (Up, Normal).

2. Перенос данных с удаляемого узла
   Перед удалением убедитесь, что данные перераспределены на другие узлы.
   Запустите команду:
		nodetool decommission

   Это безопасно удалит узел, переместив его данные на другие узлы в соответствии со стратегией репликации.

   Важно: `decommission` работает только если узел доступен и в статусе "UN".

3. Удаление "мёртвого" узла (если он уже недоступен)
   Если узел уже выключен или не отвечает, используйте:

		nodetool removenode <UUID>
   
   Чтобы найти UUID удаляемого узла:

		nodetool gossipinfo

   Найдите секцию `HOST_ID` для удаляемого узла.

4. Очистка остаточных данных на других узлах
   После удаления узла запустите cleanup на всех оставшихся узлах:

		nodetool cleanup

   Это удалит данные, которые больше не принадлежат оставшимся узлам.

5. Удаление узла из конфигурации
   Удалите его из списка seeds в файле cassandra.yaml на всех узлах.
   Убедитесь, что в system.peers его больше нет:
		
		SELECT * FROM system.peers;

6. Перезапуск оставшихся узлов
   После удаления узла перезапустите все оставшиеся узлы:
		
		sudo systemctl restart cassandra

   И снова проверьте статус:

		nodetool status

Итого:
- Если узел активен > `nodetool decommission`
- Если узел недоступен > `nodetool removenode <UUID>`
- Очистить данные на оставшихся узлах > `nodetool cleanup`
- Удалить узел из `cassandra.yaml` и `system.peers`
- Перезапустить кластер и проверить `nodetool status`

После этого узел будет полностью удалён из кластера без сбоев.  